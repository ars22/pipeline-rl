defaults:
  - finetune: base 
  - rewards: pure_success
  - streams: files
  - _self_

finetune:
  use_flash_attention: true
  attn_implementation: flash_attention_2
  config_name: ${..model_path}
  output_dir: ${..output_dir}/finetune
  seq_length: 12000
  seq_packing: true
  seq_parallel: 1
  rl:
    algo: reinforce
    divide_advantage_by_std: false
    kl_coef: 0.0
    entropy_bonus: 0.0
    reward_minus_kl_coef: 0.0
    epsilon: 4
    use_advantages: true
    relu_log_p_weights: false
    clamp_log_ratio_ref_new_value: 5
    temperature: ${...llm.parameters.temperature}
    aggregate_loss: sum
  train_batch_size: 1
  gradient_accumulation_passes: 1024
  gradient_checkpointing: true
  gradient_clipping_threshold: 0.3
  # see https://medium.com/pytorch/how-activation-checkpointing-enables-scaling-up-training-deep-learning-models-7a93ae01ff2d
  # for the motivation to this false by default
  reentrant_checkpointing: false
  learning_rate: 1e-6
  save_checkpoint_steps: 100
  log_each_n_steps: 1
  input: training_data
  send_weight_updates: true
  queue_size: 32
  max_lag: ${..max_lag}
  weight_update_interval: 1
  pop_old_data: ${..pop_old_data}
actor:
  log_each_n_secs: 0
  llm_max_rollouts: 64
  rollout_workers: 1
  discount_factor: 1
  problem_queue_size: 64
  result_queue_size: 64
  throughput_window_size: 50
  shared_memory_entry_size: 10000000
environment: null
preprocess:
  input: actor
  output: training_data
  n_workers: 8
  raw_queue_size: 8
  input_queue_size: 32
  output_queue_size: 32
  ring_buffer_size: 128
  max_unconsumed_samples: 64
  chunk_n_groups: 2
  submit_delay: 0.
  pop_old_data: ${..pop_old_data} 
  buffer_size: 0
  shared_memory_entry_size: 100000000
llm:
  parameters:
    # changed
    max_tokens: 8192
    # changed
    temperature: 1.0
test_llm:
  parameters: 
    max_tokens: 16000
    temperature: 1.0
    top_p: 0.95
    top_k: 50

vllm_config:
  use_v1: false
  vllm_kwargs:
    dtype: bfloat16
    gpu-memory-utilization: 0.9
    num-scheduler-steps: 1
    disable-log-requests: ""
    disable-frontend-multiprocessing: ""
    max-num-seqs: ${actor.llm_max_rollouts}
    max-num-batched-tokens: 1024
    enable-chunked-prefill: ""
    return-tokens-as-token-ids: ""
    tensor-parallel-size: 1
    pipeline-parallel-size: 1
    generation-config: vllm

world:
  replicas: 1
  
  actor_fraction: 4
  preprocessor_fraction: 0
  finetune_fraction: 4

  env_replicas: 2

  actor_group_port: 9000
  environment_start_port: 7777
# this will be autocreated based on the config
jobs: []

eval_every_n_versions: 78000

# changed
model_path: Qwen/Qwen2.5-7B

# will use default based on the chosen backend
accelerate_config: null
use_deepspeed: true
deepspeed_config: deepspeed_stage3_bf16
use_fsdp: false
fsdp:
  param_dtype: fp32
  reduce_dtype: fp32
  buffer_dtype: fp32

output_dir: ???
force_restart: false
pop_old_data: true
max_lag: null
attempts: 8
train_subset: null
debug:
  mode: ""
  streams_from: null
  place_inference_workers: true
  use_existing_llms: false

me:
  # Which job is this one? This will be autopopulated
  job_idx: null

hydra:
  run:
    dir: ${output_dir}

wandb:
  use_wandb: true
  wandb_id: null
  wandb_name: null
  # W&B entity name
  wandb_entity_name: null
  # W&B project name
  wandb_project_name: pipeline-rl
  # W&B resume policy
  wandb_resume: always
  # Whether to use only the basename or the full path as the run name
  wandb_use_basename: True
  wandb_workspace_root: results
  # set the group in your config
  wandb_group: null
  wandb_dir: null
  # Comma-separated list of keywords to tag the run.
  tags: []

