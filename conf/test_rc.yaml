# Test configuration for RC Actor
# Minimal config for testing online reasoning/summarization rollouts
# Usage: python test_rc_actor.py --config-name test_rc

# Inherit from base
defaults:
  - base
  - _self_


model_path: /root/pipeline-rl/models/Qwen3-4B-Thinking-2507-Proof-v03.00-step-000500 
# Qwen/Qwen3-4B-Instruct-2507

# Optional: Use a different (smaller/faster) model for summarization
# Uncomment to enable separate summarization model
summarization_model_path: Qwen/Qwen3-4B-Instruct-2507

# Random seed
seed: 42

# Output directory for test runs
output_dir: ./runs/test_rc

# vLLM configuration for generation/actor LLMs
vllm_config:
  use_v1: false
  vllm_kwargs:
    dtype: bfloat16
    gpu-memory-utilization: 0.9
    num-scheduler-steps: 1
    disable-log-requests: ""
    disable-frontend-multiprocessing: ""
    max-num-seqs: 128
    max-num-batched-tokens: 256000
    no-enable-prefix-caching: ""
    enable-chunked-prefill: ""
    return-tokens-as-token-ids: ""
    generation-config: vllm
    tensor-parallel-size: 1
    max-model-len: 131072  # Set explicit context limit (128K tokens)

# vLLM configuration for summarization LLMs 
summarization_vllm_config:
  use_v1: false
  vllm_kwargs:
    enforce-eager: ""  # for summarization LLMs, to avoid TMA issues when trying to set aside GPU memory for CUDA optimizations.
    dtype: bfloat16
    gpu-memory-utilization: 0.9
    num-scheduler-steps: 1
    disable-log-requests: ""
    disable-frontend-multiprocessing: ""
    max-num-seqs: 128
    max-num-batched-tokens: 256000  
    no-enable-prefix-caching: ""
    enable-chunked-prefill: ""
    return-tokens-as-token-ids: ""
    generation-config: vllm
    tensor-parallel-size: 1
    max-model-len: 131072  # Set explicit context limit (128K tokens)

# Test world configuration (simplified version of world config)
# Controls GPU allocation for different LLM types
test_world:
  actor_fraction: 7  # Number of LLMs for actor/generation LLMs
  summarization_fraction: 1  # Number of LLMs for summarization LLMs (0 = use same LLMs as actor)
  env_replicas: 2
  environment_start_port: 7777
  actor_group_port: 9000

# RC Actor configuration
actor:
  # Number of reasoning/summarization cycles per problem
  num_reasoning_steps: 8
  
  # Rollout policies
  solution_rollout_policy: pipelinerl.domains.math.rollouts.generate_math_rollout
  summarization_rollout_policy: pipelinerl.domains.math.rollouts.generate_summarization_rollout
  system_prompt: null
  # Please reason step by step, and put your final answer within \boxed{}.
  
  # Prompt template files (loaded from files)
  reasoning_prompt_file: prompts/rc_reasoning_prompt.txt
  summarization_prompt_file: prompts/rc_summarization_prompt.txt
  
  # Task template for formatting the problem
  task_template: |-
    {task}
  
  # Rollout configuration
  rollout_workers: 1
  problem_queue_size: 128
  result_queue_size: 128
  shared_memory_entry_size: 15728640  # 15MB
  throughput_window_size: 10
  
  # Retry configuration
  max_retries: 3
  retry_base_delay: 1.0

# LLM generation parameters for reasoning/solution generation
llm:
  parameters:
    temperature: 0.6  
    max_tokens: 32768 # 32768  # Longer reasoning chains
    top_p: 0.95
    top_k: 20

# LLM generation parameters for summarization 
summarization_llm:
  parameters:
    temperature: 0.7  
    max_tokens: 2048  # Shorter summaries
    top_p: 0.8
    top_k: 20

test_llm:
  parameters:
    temperature: 0.6
    max_tokens: 32768 # 32768
    top_p: 0.95
    top_k: 20

# Streams backend
streams:
  backend: files

# Disable WandB for testing
wandb:
  use_wandb: false

# Training attempts (for online RC, we use 1 for training and higher values for evaluation)
attempts: 4

# Disable eval during test
eval_every_n_versions: 0

# Eval only mode
eval_only: true

# Debug mode
debug:
  mode: true

# Finetune config (minimal for state management)
finetune:
  max_lag: null


# Dataset configuration - using POPE hard dataset
dataset_loader: pipelinerl.domains.math.load_datasets
train_dataset_names:
  - imo_proof_bench
  # - hub_id: hf-imo-colab/olympiads-proof-schema-benchmark
  #   split: IMO2025
  # - imo_answer_bench
test_dataset_names:
  - imo_proof_bench
  # - hub_id: hf-imo-colab/olympiads-proof-schema-benchmark
  #   split: IMO2025
  # - imo_answer_bench

# Environment configuration 
# environment:
#   _target_: pipelinerl.domains.math.MathEnvironment

environment:
  _target_: pipelinerl.domains.math.MathProofEnvironment
  model_name: ${llm_grader.name}
  sampling_kwargs: ${llm_grader.sampling_kwargs}
  prompt_name: ${llm_grader.prompt_name}

llm_grader:
  name: openai/gpt-oss-20b # gpt-oss-20b-reh # or gpt-oss-120b-twj if running via inference endpoints
  sampling_kwargs:
    temperature: 1.0
    max_output_tokens: 81920
    reasoning:
      effort: medium
  # reasoning_delimiters: ["</think>"]
  prompt_name: v1

