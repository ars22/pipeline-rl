# Test configuration for RC Actor
# Minimal config for testing online reasoning/summarization rollouts
# Usage: python test_rc_actor.py --config-name test_rc

# Inherit from base
defaults:
  - base
  - _self_

# Model - use small model for fast testing
model_path: Qwen/Qwen3-4B-Instruct-2507

# Random seed
seed: 42

# Output directory for test runs
output_dir: ./runs/test_rc

# vLLM configuration
vllm_config:
  use_v1: false
  vllm_kwargs:
    dtype: bfloat16
    gpu-memory-utilization: 0.95
    num-scheduler-steps: 16
    disable-log-requests: ""
    disable-frontend-multiprocessing: ""
    max-num-seqs: ${actor.llm_max_rollouts}
    max-num-batched-tokens: 64000
    no-enable-prefix-caching: ""
    enable-chunked-prefill: ""
    return-tokens-as-token-ids: ""
    generation-config: vllm

# Test world configuration (simplified version of world config)
# Only actor_fraction is used for testing - determines GPU allocation for LLM servers
test_world:
  actor_fraction: 2  # Use 100% of available GPUs for actor LLMs
  env_replicas: 2
  environment_start_port: 7777
  actor_group_port: 9000

# RC Actor configuration
actor:
  # Number of reasoning/summarization cycles per problem
  num_reasoning_steps: 2
  
  # Rollout policies
  solution_rollout_policy: pipelinerl.domains.math.rollouts.generate_math_rollout
  summarization_rollout_policy: pipelinerl.domains.math.rollouts.generate_summarization_rollout
  system_prompt: Please reason step by step, and put your final answer within \boxed{}.
  task_template: |-
    {task}


  # Prompt templates
  reasoning_prompt_template: |
    Solve this problem step by step.
    
    Problem: {problem}
    
    Previous work: {curr_summary}
    
    Continue your reasoning:
  
  summarization_prompt_template: |
    Summarize the solution progress.
    
    Problem: {problem}
    
    Previous summary: {existing_summary}
    
    New reasoning: {reasoning}
    
    Provide a concise updated summary:
  
  # Rollout configuration
  rollout_workers: 1
  llm_max_rollouts: 16
  problem_queue_size: 10
  result_queue_size: 10
  shared_memory_entry_size: 5242880  # 5MB
  throughput_window_size: 10
  
  # Retry configuration
  max_retries: 3
  retry_base_delay: 1.0

# LLM generation parameters
llm:
  parameters:
    temperature: 1.0
    max_tokens: 16384
    top_p: 0.95

test_llm:
  parameters:
    temperature: 1.0
    max_tokens: 16384
    top_p: 1.0

# Streams backend
streams:
  backend: files

# Disable WandB for testing
wandb:
  use_wandb: false

# Training attempts (for online RC, use 1)
attempts: 1

# Disable eval during test
eval_every_n_versions: 0

# Debug mode
debug:
  mode: true

# Finetune config (minimal for state management)
finetune:
  max_lag: null


# Dataset configuration - using POPE hard dataset
dataset_loader: pipelinerl.domains.math.load_datasets
train_dataset_names:
  - pope_512
test_dataset_names:
  - pope_512

# Environment configuration 
environment:
  _target_: pipelinerl.domains.math.MathEnvironment

# LLM Grader configuration
llm_grader:
  name: openai/gpt-oss-20b
  vllm_kwargs:
    num_nodes: 2                    # number of nodes for the grader server
    data-parallel-size: 16          # data parallelism
    tensor-parallel-size: 1         # tensor parallelism
    max-num-batched-tokens: 8192    # tokenizer budget per batch
    max-num-seqs: 16                # concurrent sequences
    max-model-len: 32768            # prompt + output budget
    gpu-memory-utilization: 0.85    # fraction of GPU memory vLLM can use

